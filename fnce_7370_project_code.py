# -*- coding: utf-8 -*-
"""FNCE 7370 - Project Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l-upPLvxbRp6zb7XN-VjpUIGJ_MeroVw

TODO
TODO: Let's also remove reviews that occurred much before the data we have for employee counts in `workforce_df`
1. Create shared folder and upload datasets to there
2. Import into colab 
3. Maybe create keyword wordcloud for mid project pres?
4. Plot distribution of reviews ratings (the number's, not the words) to see if its skewed or balanced.
5. Maybe get top 5 or bottom 5 companies (based on employee reviews)
6. Filter out companies with < 20 reviews. 
7. Try to see the intersection between the employee-reviews-dataset companies and the workforce-dynamics-geo companies. Join both datasets.
8. Try to maybe run a regression of review rating (the number) -> change in count (in the next month). OLS. 1 x-variable = review star-rating (the number out of 5).

#Section 1: Setup environment and install packages
"""

import pandas as pd 
import numpy as np
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm
from google.colab import data_table
import gc

from google.colab import drive
drive.mount('/content/gdrive')

wd = 'gdrive/Shareddrives/737_project/'
revelio_wd = wd + 'revelio_data/'

"""#Section 2: Import, clean and shrink workforce data

In this section, we aim to clean the worforce dataset and shrink its size, so that we can manipulate it more easily in the future since Colab only has so few resources that we have access to.
"""

workforce_df = pd.read_csv(revelio_wd + 'revelio_wf_dynam_geo_breakdown.csv')

num_na_rows = workforce_df.isna().sum().sum()
print("The data has {0} rows with na".format(num_na_rows))

"""No need to remove any na rows since there aren't any, so let's drop columns that we don't need."""

workforce_df.columns

drop_cols = ['STATE', 'MSA']
workforce_df.drop(columns=drop_cols, inplace=True)
workforce_df.shape

workforce_df.head(3)

"""The dataset has 16 million rows! Let's figure out which ones we need to delete. To do that, let's group by company and sum the counts of employees and number of months we have data for to see which companies we can remove from the dataset (e.g., we might want to remove companies with less than 23 months of data, or less than average 10 employees). The float values in some of the columns are expected because Revelio uses math and percentages to estimate count, inflow and outflow. """

#Need to create dfs to figure out which companies to remove
company_to_emp_count_df = workforce_df[['COMPANY', 'MONTH', 'COUNT']].groupby(by=['COMPANY', 'MONTH'], as_index=False).sum()
company_avg_emp_count_df = company_to_emp_count_df[['COMPANY', 'COUNT']].groupby(by='COMPANY').mean()
company_num_months_df = company_to_emp_count_df[['COMPANY', 'MONTH']].groupby(by='COMPANY').count()

companies_to_remove = []
companies_to_remove.extend(list(company_num_months_df[company_num_months_df['MONTH'] < 23].index))
companies_to_remove.extend(list(company_avg_emp_count_df[company_avg_emp_count_df['COUNT'] < 10].index))
companies_to_remove = list(set(companies_to_remove)) #Remove duplicates
print("Need to remove {0} companies from workforce_df".format(len(companies_to_remove)))

"""Now let's set `workforce_df` to a group-by'd version where we sum the count's, inflow's, outflow's and average the salary colum. Then let's remove the `companies_to_remove`."""

workforce_df = workforce_df.groupby(by=['COMPANY', 'MONTH'], as_index=False).agg(
    count=pd.NamedAgg(column="COUNT", aggfunc="sum"),
    inflow=pd.NamedAgg(column="INFLOW", aggfunc="sum"),
    outflow=pd.NamedAgg(column="OUTFLOW", aggfunc="sum"),
    salary=pd.NamedAgg(column="SALARY", aggfunc="mean"))
workforce_df.shape

workforce_df = workforce_df[~workforce_df['COMPANY'].isin(companies_to_remove)]
workforce_df.shape

"""We've successfully reduced the size of the dataset to 155k rows! Now we can clear some variables to make space for colab."""

del company_to_emp_count_df
del company_avg_emp_count_df
del company_num_months_df
del companies_to_remove

gc.collect()

"""#Section 3: Import, clean and shrink glassdoor data

In this section, we aim to clean the glassdoor dataset and shrink its size, so that we can manipulate it more easily in the future since Colab only has so few resources that we have access to.
"""

#Takes 1 min to run
glassdoor_df = pd.read_csv(revelio_wd + 'revelio_glassdoor_company_reviews.csv')

glassdoor_df.columns

glassdoor_df.shape

"""There are 5.4 million rows in the dataset across 23 columns. Let's drop unnecessary columns to shrink size of dataset. Also let's drop any rows with an na value."""

drop_cols = ['JOB_TITLE_NAME', 'CITY_NAME']
glassdoor_df.drop(columns=drop_cols, inplace=True)
glassdoor_df.dropna(inplace=True)
glassdoor_df.shape

"""Now let's remove reviews for companies that do not meet a threshold amount of reviews/company (e.g., we don't care about companies that have less than 10 reviews per company, since we can't do anything meaningful with the data anyways)."""

glass_company_to_num_reviews_df = glassdoor_df[['COMPANY_NAME', 'REVIEW_DATE_TIME']].groupby(by='COMPANY_NAME').count()
glass_company_to_num_reviews_df.rename(columns={'REVIEW_DATE_TIME': 'Num_Reviews'}, inplace=True)
glass_company_to_num_reviews_df.describe()

"""Interesting that the 75th % num_reviews is 5, so let's use 5 as our threshold, and remove companies from our original dataset that don't meet the threshold number of reviews."""

GLASS_MIN_REVIEWS_PER_COMPANY_THRESHOLD = 5
glass_company_to_num_reviews_df = glass_company_to_num_reviews_df[glass_company_to_num_reviews_df['Num_Reviews'] >= GLASS_MIN_REVIEWS_PER_COMPANY_THRESHOLD]
companies_to_keep = list(glass_company_to_num_reviews_df.index)
print("We should keep {0} companies in our dataset, since they have >= {1} reviews each".format(len(companies_to_keep), GLASS_MIN_REVIEWS_PER_COMPANY_THRESHOLD))

glassdoor_df = glassdoor_df[glassdoor_df['COMPANY_NAME'].isin(companies_to_keep)]

glassdoor_df.shape

"""Great, we shrunk the dataset from 5.4 million rows to 4.8 million rows (and 2 less columns). Not as big of an improvement as in section 2 but good nonetheless. Now let's clear up some memory we don't need."""

del companies_to_keep
del glass_company_to_num_reviews_df

import gc
gc.collect()

"""#Section 4: Remove companies/reviews not in common between Glassdoor and Workforce datasets

Since we don't know how we want to combine datasets yet, let's remove companies from each dataset that are not found in the other dataset.
"""

companies_in_common = list(set(workforce_df['COMPANY'].unique()) & set(glassdoor_df['COMPANY_NAME'].unique()))
print("There are {0} companies in common between the two datasets".format(len(companies_in_common)))

print("The shape of workforce_df is {0} and shape of glassdoor_df is {1}".format(workforce_df.shape, glassdoor_df.shape))

workforce_df = workforce_df[workforce_df['COMPANY'].isin(companies_in_common)]
glassdoor_df = glassdoor_df[glassdoor_df['COMPANY_NAME'].isin(companies_in_common)]
print("The shape of workforce_df is {0} and shape of glassdoor_df is {1}".format(workforce_df.shape, glassdoor_df.shape))

"""#Section 5: Data Visualization

In this section, we try to visualize both the glassdoor and workforce datasets to get an understanding of how we can implement some kind of regression later on.
"""

#Here, we visualize the distribution of employees per company (includes repeat months)
workforce_df.hist(column='count')

#Here, we visualize the distribution of ratings per company
glassdoor_df.hist(column='RATING_OVERALL')

glassdoor_df.head(10).T

"""#Section 6: Top 30 most used words in REVIEW_PROS"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import string
import re

nltk.download('stopwords')
nltk.download('punkt')

text_column = glassdoor_df['REVIEW_PROS']
combined_text = ' '.join(text_column)
words = word_tokenize(combined_text)
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)
processed_words = [word.lower() for word in words if word.lower() not in stop_words and word not in punctuation and re.match("^[a-zA-Z0-9]*$", word)]
word_counts = Counter(processed_words)
most_common_words = word_counts.most_common(30)

print(most_common_words)

!pip install plotly

import plotly.express as px

# Extract the words and their counts from the 'most_common_words' list
words = [item[0] for item in most_common_words]
counts = [item[1] for item in most_common_words]

# Create a Pandas DataFrame from the words and counts
word_count_df = pd.DataFrame({'Word': words, 'Count': counts})

# Create the bar chart using Plotly Express
fig = px.bar(word_count_df, x='Word', y='Count', text='Count', title='Top 30 Most Common Words in REVIEW_PROS',
             labels={'Word': 'Words', 'Count': 'Frequency'},
             color='Count', color_continuous_scale='Viridis')

# Customize the appearance
fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')
fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')

# Show the interactive plot
fig.show()

!pip install wordcloud -q
!pip install matplotlib -q
!pip install mplcursors -q

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import mplcursors

# Create a dictionary from the 'most_common_words' list
word_count_dict = dict(most_common_words)

# Configure the word cloud
wordcloud = WordCloud(width=2000, height=800, background_color='white',
                      colormap='viridis', min_font_size=10).generate_from_frequencies(word_count_dict)

# Display the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.tight_layout(pad=0)

# Make the word cloud more interactive using mplcursors
mplcursors.cursor(hover=True)

# Show the word cloud
plt.show()

"""# Section 7: Top 30 most used words in REVIEW_CONS"""

text_column = glassdoor_df['REVIEW_CONS']
combined_text = ' '.join(text_column)
words = word_tokenize(combined_text)
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)
processed_words = [word.lower() for word in words if word.lower() not in stop_words and word not in punctuation and re.match("^[a-zA-Z0-9]*$", word)]
word_counts = Counter(processed_words)
most_common_words = word_counts.most_common(30)

print(most_common_words)

import plotly.express as px

# Extract the words and their counts from the 'most_common_words' list
words = [item[0] for item in most_common_words]
counts = [item[1] for item in most_common_words]

# Create a Pandas DataFrame from the words and counts
word_count_df = pd.DataFrame({'Word': words, 'Count': counts})

# Create the bar chart using Plotly Express
fig = px.bar(word_count_df, x='Word', y='Count', text='Count', title='Top 30 Most Common Words in REVIEW_CONS',
             labels={'Word': 'Words', 'Count': 'Frequency'},
             color='Count', color_continuous_scale='Viridis')

# Customize the appearance
fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')
fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')

# Show the interactive plot
fig.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import mplcursors

# Create a dictionary from the 'most_common_words' list
word_count_dict = dict(most_common_words)

# Configure the word cloud
wordcloud = WordCloud(width=2000, height=800, background_color='white',
                      colormap='viridis', min_font_size=10).generate_from_frequencies(word_count_dict)

# Display the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.tight_layout(pad=0)

# Make the word cloud more interactive using mplcursors
mplcursors.cursor(hover=True)

# Show the word cloud
plt.show()



"""# Section 8: Top 5 and Bottom 5 Companies"""

glassdoor_df.head(3)

average_ratings = glassdoor_df.groupby('COMPANY_NAME')['RATING_OVERALL'].mean()
average_ratings_df = average_ratings.to_frame()
print('TOP 5 Companies:')
print(average_ratings_df.sort_values(by='RATING_OVERALL', ascending=False).reset_index().head(5))
print('')
print('Bottom 5 Companies:')
print(average_ratings_df.sort_values(by='RATING_OVERALL', ascending=False).reset_index().tail(5))

"""# Section 9: Regression on COUNT"""

glassdoor_df['REVIEW_DATE_TIME'] = pd.to_datetime(glassdoor_df['REVIEW_DATE_TIME'])
glassdoor_df['REVIEW_DATE_TIME'] = glassdoor_df['REVIEW_DATE_TIME'].dt.strftime('%Y-%m')

workforce_df['MONTH'] = pd.to_datetime(workforce_df['MONTH'])
workforce_df['MONTH'] = workforce_df['MONTH'].dt.strftime('%Y-%m')

selected_columns = ['RATING_CAREER_OPPORTUNITIES', 'RATING_COMPENSATION_AND_BENEFITS', 
                    'RATING_CULTURE_AND_VALUES', 'RATING_DIVERSITY_AND_INCLUSION',
                    'RATING_SENIOR_LEADERSHIP', 'RATING_WORK_LIFE_BALANCE']

 # encoding                    
glassdoor_df[selected_columns] = glassdoor_df[selected_columns].replace('\\\\N', '0')

regression_glassdoor_df = glassdoor_df[['COMPANY_NAME', 'REVIEW_DATE_TIME', 'RATING_OVERALL',
                                        'RATING_CAREER_OPPORTUNITIES', 'RATING_COMPENSATION_AND_BENEFITS', 
                                        'RATING_CULTURE_AND_VALUES', 'RATING_DIVERSITY_AND_INCLUSION',
                                        'RATING_SENIOR_LEADERSHIP', 'RATING_WORK_LIFE_BALANCE']]

rating_columns = [
    'RATING_OVERALL',
    # 'RATING_CAREER_OPPORTUNITIES',
    # 'RATING_COMPENSATION_AND_BENEFITS',
    'RATING_CULTURE_AND_VALUES',
    'RATING_DIVERSITY_AND_INCLUSION',
    # 'RATING_SENIOR_LEADERSHIP',
    # 'RATING_WORK_LIFE_BALANCE',
]

for column in rating_columns:
    regression_glassdoor_df[column] = pd.to_numeric(regression_glassdoor_df[column], errors='coerce')

#For each company-month, average the ratings to one rating
regression_glassdoor_df = regression_glassdoor_df.groupby(['COMPANY_NAME', 'REVIEW_DATE_TIME'])[rating_columns].mean().reset_index()
regression_glassdoor_df = regression_glassdoor_df[regression_glassdoor_df['REVIEW_DATE_TIME'] >= '2021-01']

regression_glassdoor_df.head(10).T

workforce_df['count'] = pd.to_numeric(workforce_df['count'], errors='coerce')
workforce_df['count_pct_change'] = workforce_df['count'].pct_change() * 100
workforce_df = workforce_df[workforce_df['MONTH'] != '2021-01'] #new line

workforce_df['lagged_count_pct_change'] = workforce_df.groupby(by=['COMPANY'])['count_pct_change'].shift(1)
workforce_df['future_count_pct_change'] = workforce_df.groupby(by=['COMPANY'])['count_pct_change'].shift(-1)
workforce_df = workforce_df[workforce_df['MONTH'] > '2021-02'] #Remove companies in the first 2 months of 2021 because lagged vars don't exist

merged_regression = pd.merge(regression_glassdoor_df, workforce_df, 
                       left_on=['COMPANY_NAME', 'REVIEW_DATE_TIME'], 
                       right_on=['COMPANY', 'MONTH'])

drop_cols = ['COMPANY', 'MONTH', 'count', 'inflow', 'outflow', 'salary']
merged_regression.drop(columns=drop_cols, inplace=True)

# merged_regression =

merged_regression = merged_regression.groupby(['COMPANY_NAME', 'REVIEW_DATE_TIME']).agg(lambda x: x.mean()).reset_index()

merged_regression

print("Num na's from lagged columns: " + str(merged_regression.isna().sum().sum()))
merged_regression.dropna(inplace=True)
print("Num na's: " + str(merged_regression.isna().sum().sum()))

merged_regression

# Regression using only RATING_OVERALL

X_ols1 = merged_regression['RATING_OVERALL']
y = merged_regression['count_pct_change']

X_ols1 = sm.add_constant(X_ols1)
ols_model = sm.OLS(y, X_ols1).fit()
print(ols_model.summary())

merged_regression

# Regression using 9 features 

feature_columns = [
    'RATING_OVERALL', #Exclude certain rows because they are already incorporated in raitng overall!
    # 'RATING_CAREER_OPPORTUNITIES',
    # 'RATING_COMPENSATION_AND_BENEFITS',
    'RATING_CULTURE_AND_VALUES',
    'RATING_DIVERSITY_AND_INCLUSION',
    # 'RATING_SENIOR_LEADERSHIP',
    # 'RATING_WORK_LIFE_BALANCE',
    'lagged_count_pct_change'
]

X_ols2 = merged_regression[feature_columns]
# X_ols2['lagged_count_pct_change'].fillna(X_ols2['lagged_count_pct_change'].mean(), inplace = True)
y = merged_regression['count_pct_change']

X_ols2 = sm.add_constant(X_ols2)
ols_model_2 = sm.OLS(y, X_ols2).fit()
print(ols_model_2.summary())

merged_regression['lagged_count_pct_change'].fillna(merged_regression['lagged_count_pct_change'].mean(), inplace = True)
merged_regression['future_count_pct_change'].fillna(merged_regression['future_count_pct_change'].mean(), inplace = True)

# Regression using 9 features to predict count of 2 MONTHS ahead!! 

feature_columns = [
    'RATING_OVERALL',
    'RATING_CULTURE_AND_VALUES',
    'RATING_DIVERSITY_AND_INCLUSION',
    'lagged_count_pct_change'
]

X_ols2 = merged_regression[feature_columns]
y = merged_regression['future_count_pct_change']

X_ols2 = sm.add_constant(X_ols2)
ols_model_2 = sm.OLS(y, X_ols2).fit()
print(ols_model_2.summary())

merged_regression[merged_regression['count_pct_change'] > 0]['count_pct_change'] = 1
merged_regression[merged_regression['count_pct_change'] < 0]['count_pct_change'] = 0

temp_copy = merged_regression.copy()
temp_copy.head(8).T

feature_columns

"""# Section 10: Random Forest Classifier"""



X_train_rf

"""# Section 11: Adaboost

"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV

adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)
adaboost.fit(X_train_rf, y_train_rf)
adaboost.score(X_train_rf, y_train_rf)

scores = []
for i in [10, 30, 50, 80, 100]:
  adaboost = AdaBoostClassifier(n_estimators=i, random_state=0)
  adaboost.fit(X_train_rf, y_train_rf)
  score = adaboost.score(X_test_rf, y_test_rf)
  scores.append(score)

x = [10, 30, 50, 80, 100]
plt.plot(x, scores)
plt.xlabel('number of estimators')
plt.ylabel('test accuracy')

adaboost = AdaBoostClassifier(n_estimators=50, random_state=0)
adaboost.fit(X_train_rf, y_train_rf)
y_pred_rf = adaboost.predict(X_test_rf)

accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)
print("Accuracy:", accuracy_rf)

report_rf = classification_report(y_test_rf, y_pred_rf)
print("Classification Report:\n", report_rf)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test_rf, y_pred_rf)
print("Confusion Matrix:\n", cm)

import plotly.figure_factory as ff
def plot_confusion_matrix(cm, labels):
    z = cm
    x = labels
    y = labels
    
    fig = ff.create_annotated_heatmap(z, x=x, y=y, colorscale='Viridis')
    fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted', yaxis_title='True')
    
    return fig

labels = ['Negative', 'Positive']
fig = plot_confusion_matrix(cm, labels)
fig.show()

"""# Section 12: NLP models (count_pct_change) """

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer

regression_glassdoor_df = glassdoor_df

merged_regression = pd.merge(regression_glassdoor_df, workforce_df, 
                       left_on=['COMPANY_NAME', 'REVIEW_DATE_TIME'], 
                       right_on=['COMPANY', 'MONTH'])
drop_cols = ['COMPANY', 'MONTH', 'count', 'inflow', 'outflow', 'salary']
merged_regression.drop(columns=drop_cols, inplace=True)

merged_regression.columns

merged_regression[merged_regression['count_pct_change'] > 0]['count_pct_change'] = 1
merged_regression[merged_regression['count_pct_change'] < 0]['count_pct_change'] = 0

# merged_regression.drop(['lagged_count_pct_change', 'future_count_pct_change'], axis = 1, inplace = True)

merged_regression.drop(['future_count_pct_change'], axis = 1, inplace = True)

merged_regression.drop_duplicates(inplace = True)

merged = merged_regression.groupby(['COMPANY_NAME', 'REVIEW_DATE_TIME']).agg(lambda x: x.tolist()).reset_index()

merged['y'] = merged['count_pct_change'].apply(lambda x: x[0])

merged.drop('count_pct_change', axis = 1, inplace = True)

merged['review'] = pd.concat([merged['REVIEW_CONS'], merged['REVIEW_PROS']], ignore_index=True)

merged['token'] = merged['review'].apply(lambda x: word_tokenize(str(x).lower()))
merged.loc[merged['y'] > 0, 'y'] = 1
merged.loc[merged['y'] <= 0, 'y'] = 0

# Remove Stop words, Non-Numeric and perfom Word Lemmenting
# WordNetLemmatizer requires part of speech tags to understand if the word is noun or verb or adjective etc. 
# By default it is set to Noun using the defaultdict 
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV
for index,entry in enumerate(merged['token']):
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words = []
    # Initializing WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
    # pos_tag returns a list of tuples
    for word, tag in pos_tag(entry):
           # Below condition is to check for Stop words and consider only alphabet characters
            if word not in stopwords.words('english') and word.isalpha():
                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
                Final_words.append(word_Final)
    # The final processed set of words for each iteration is stored in a new column of the DataFrame called 'text_final'
    merged.loc[index,'text_final'] = str(Final_words)

"""## SVM Model"""

from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

mean_svm_score = 0 

column_list = []
   
#test_size specifies the size of the testing data set 
Train_X, Test_X, Train_Y, Test_Y = train_test_split(merged['text_final'], merged['y'],test_size=0.3)

#max_features = 5000 --> builds a vocabulary that only considers the top max_fearures ordered by term frequency across the corpus
Tfidf_vect = TfidfVectorizer(max_features=5000)
#.fit learns vocabular and idf from training set 
Tfidf_vect.fit(merged['text_final'])
#.transform transforms documents to document-term matrix using the vocabulary and df learned by .fit
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

#Step 8 --  Use the ML Algorithms to Predict the outcome

# Classifier - Algorithm - SVM
# fit the training dataset on the classifier
SVM = SVC(C=1.0, kernel='linear', degree=2, gamma= 'auto')
SVM.fit(Train_X_Tfidf, Train_Y)
print(SVM.score(Train_X_Tfidf, Train_Y))

# predict the labels on validati on dataset
predictions_SVM = SVM.predict(Test_X_Tfidf)

# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score: ",accuracy_score(predictions_SVM, Test_Y)*100)
print("SVM Explained Variance Score: ",explained_variance_score(Test_Y, predictions_SVM, multioutput='uniform_average'))

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
 
kernels = ['rbf','linear','poly','sigmoid']
# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': ['scale', 'auto'],
              'kernel': ['rbf']}
 
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
 
# fitting the model for grid search
grid.fit(Train_X_Tfidf,Train_Y)

# print best parameter after tuning
print(grid.best_params_)
 
# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

grid_predictions = grid.predict(Test_X_Tfidf)
 
# print classification report
print(classification_report(Test_Y, grid_predictions))

SVM = SVC(C=0.1, kernel='rbf', gamma= 'scale')
SVM.fit(Train_X_Tfidf,Train_Y)
print(SVM.score(Train_X_Tfidf, Train_Y))
# predict the labels on validati on dataset
predictions_SVM = SVM.predict(Test_X_Tfidf)

# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score: ",accuracy_score(predictions_SVM, Test_Y)*100)

"""## NB Model"""

from sklearn.naive_bayes import MultinomialNB
#Step 8 --  Use the ML Algorithms to Predict the outcome
# fit the training dataset on the NB classifier
Naive = MultinomialNB()
Naive.fit(Train_X_Tfidf,Train_Y)
# predict the labels on validation dataset
predictions_NB = Naive.predict(Test_X_Tfidf)
print(Naive.score(Train_X_Tfidf, Train_Y))

# Use accuracy_score function to get the accuracy
print("Naive Bayes Accuracy Score: ",accuracy_score(predictions_NB, Test_Y)*100)
print("Naive Bayes Explained Variance Score: ",explained_variance_score(Test_Y, predictions_NB, multioutput='uniform_average'))

"""## RF Model

"""

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(Train_X_Tfidf, Train_Y)
print(clf.score(Train_X_Tfidf, Train_Y))

predictions_RF = clf.predict(Test_X_Tfidf)
print("Random Forest with 10 Estimators Accuracy Score: ",accuracy_score(predictions_RF, Test_Y)*100)
print("Random Forest Explained Variance Score: ",explained_variance_score(Test_Y, predictions_RF, multioutput='uniform_average'))

"""## Neural Net """

from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

column_list = []

# Split the dataset into training and testing sets
Train_X, Test_X, Train_Y, Test_Y = train_test_split(merged['text_final'], merged['y'], test_size=0.3)

# Convert the text data into TF-IDF features
Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(merged['text_final'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

# Classifier - Algorithm - Neural Network (MLP)
nn = MLPClassifier(hidden_layer_sizes=(32, 128, 16), activation='relu', solver='adam', random_state=42)
nn.fit(Train_X_Tfidf, Train_Y)
print(nn.score(Train_X_Tfidf, Train_Y))


# Predict the labels on the validation dataset
predictions_NN = nn.predict(Test_X_Tfidf)

# Use accuracy_score function to get the accuracy
print("Neural Network Accuracy Score: ", accuracy_score(predictions_NN, Test_Y) * 100)
print("Neural Network Explained Variance Score: ", explained_variance_score(Test_Y, predictions_NN, multioutput='uniform_average'))

"""## Logistic Regression"""

from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

column_list = []

# Split the dataset into training and testing sets
Train_X, Test_X, Train_Y, Test_Y = train_test_split(merged['text_final'], merged['y'], test_size=0.3)

# Convert the text data into TF-IDF features
Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(merged['text_final'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

# Classifier - Algorithm - Logistic Regression
lr = LogisticRegression(random_state=42)
lr.fit(Train_X_Tfidf, Train_Y)
print(lr.score(Train_X_Tfidf, Train_Y))

# Predict the labels on the validation dataset
predictions_LR = lr.predict(Test_X_Tfidf)

# Use accuracy_score function to get the accuracy
print("Logistic Regression Accuracy Score: ", accuracy_score(predictions_LR, Test_Y) * 100)
print("Logistic Regression Explained Variance Score: ", explained_variance_score(Test_Y, predictions_LR, multioutput='uniform_average'))

## majortiy vote 

predictions_svm = SVM.predict(Test_X_Tfidf)
predictions_nb = Naive.predict(Test_X_Tfidf)
predictions_rf = clf.predict(Test_X_Tfidf)
predictions_nn = nn.predict(Test_X_Tfidf)
predictions_lr = lr.predict(Test_X_Tfidf)

from scipy.stats import mode

combined_predictions = []
for i in range(len(predictions_svm)):
    combined_prediction = mode([predictions_svm[i], predictions_nb[i], predictions_rf[i], predictions_nn[i], predictions_lr[i]])[0][0]
    combined_predictions.append(combined_prediction)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(Test_Y, combined_predictions)
print("Accuracy using Majority Vote:", accuracy)

weights = [explained_variance_score(Test_Y, predictions_svm, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_nb, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_rf, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_nn, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_lr, multioutput='uniform_average')]

weighted_predictions = []
for i in range(len(predictions_svm)):
    vote_count = {}
    for j, prediction in enumerate([predictions_svm[i], predictions_nb[i], predictions_rf[i], predictions_nn[i], predictions_lr[i]]):
        if prediction in vote_count:
            vote_count[prediction] += weights[j]
        else:
            vote_count[prediction] = weights[j]
    weighted_prediction = max(vote_count, key=vote_count.get)
    weighted_predictions.append(weighted_prediction)

accuracy = accuracy_score(Test_Y, weighted_predictions)
print("Accuracy using Weighted Majority Vote:", accuracy)

# Print the weights assigned to each model
print("Weights assigned to each model:", weights)

# Determine the index of the model with the highest weight
most_important_model_index = weights.index(max(weights))

# Print the name of the most important model
model_names = ["SVM", "Naive Bayes", "Random Forest", "Neural Network", "Logistic Regression"]
print("The most important model is:", model_names[most_important_model_index])

"""# Section 13: Repeat all NLP models for future_count_pct_change """

regression_glassdoor_df = glassdoor_df

merged_regression = pd.merge(regression_glassdoor_df, workforce_df, 
                       left_on=['COMPANY_NAME', 'REVIEW_DATE_TIME'], 
                       right_on=['COMPANY', 'MONTH'])
drop_cols = ['COMPANY', 'MONTH', 'count', 'inflow', 'outflow', 'salary']
merged_regression.drop(columns=drop_cols, inplace=True)

merged_regression[merged_regression['future_count_pct_change'] > 0]['future_count_pct_change'] = 1
merged_regression[merged_regression['future_count_pct_change'] < 0]['future_count_pct_change'] = 0 

merged_regression.drop(['count_pct_change'], axis = 1, inplace = True)

merged_regression.drop_duplicates(inplace = True)

merged = merged_regression.groupby(['COMPANY_NAME', 'REVIEW_DATE_TIME']).agg(lambda x: x.tolist()).reset_index()

merged['y'] = merged['future_count_pct_change'].apply(lambda x: x[0])

merged.drop('future_count_pct_change', axis = 1, inplace = True)

merged['review'] = pd.concat([merged['REVIEW_CONS'], merged['REVIEW_PROS']], ignore_index=True)

merged['token'] = merged['review'].apply(lambda x: word_tokenize(str(x).lower()))
merged.loc[merged['y'] > 0, 'y'] = 1
merged.loc[merged['y'] <= 0, 'y'] = 0

feature_columns = [
    'RATING_OVERALL', 
    'RATING_CAREER_OPPORTUNITIES',
    'RATING_COMPENSATION_AND_BENEFITS',
    'RATING_SENIOR_LEADERSHIP',
    'RATING_WORK_LIFE_BALANCE',
    'lagged_count_pct_change',
    'text_final'
]

# Remove Stop words, Non-Numeric and perfom Word Lemmenting
# WordNetLemmatizer requires part of speech tags to understand if the word is noun or verb or adjective etc. 
# By default it is set to Noun using the defaultdict 
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV
for index,entry in enumerate(merged['token']):
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words = []
    # Initializing WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
    # pos_tag returns a list of tuples
    for word, tag in pos_tag(entry):
           # Below condition is to check for Stop words and consider only alphabet characters
            if word not in stopwords.words('english') and word.isalpha():
                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
                Final_words.append(word_Final)
    # The final processed set of words for each iteration is stored in a new column of the DataFrame called 'text_final'
    merged.loc[index,'text_final'] = str(Final_words)

merged.dropna(subset=['y'], inplace=True)

from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
mean_svm_score = 0 

column_list = []
   
#test_size specifies the size of the testing data set 
Train_X, Test_X, Train_Y, Test_Y = train_test_split(merged['text_final'], merged['y'],test_size=0.3)

#max_features = 5000 --> builds a vocabulary that only considers the top max_fearures ordered by term frequency across the corpus
Tfidf_vect = TfidfVectorizer(max_features=5000)
#.fit learns vocabular and idf from training set 
Tfidf_vect.fit(merged['text_final'])
#.transform transforms documents to document-term matrix using the vocabulary and df learned by .fit
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

#Step 8 --  Use the ML Algorithms to Predict the outcome

# Classifier - Algorithm - SVM
# fit the training dataset on the classifier
SVM = SVC(C=1.0, kernel='linear', degree=2, gamma= 'auto')
SVM.fit(Train_X_Tfidf, Train_Y)

# predict the labels on validati on dataset
predictions_SVM = SVM.predict(Test_X_Tfidf)
print(SVM.score(Train_X_Tfidf, Train_Y))

# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score: ",accuracy_score(predictions_SVM, Test_Y)*100)
print("SVM Explained Variance Score: ",explained_variance_score(Test_Y, predictions_SVM, multioutput='uniform_average'))

print ("")

from sklearn.naive_bayes import MultinomialNB
#Step 8 --  Use the ML Algorithms to Predict the outcome
# fit the training dataset on the NB classifier
Naive = MultinomialNB()
Naive.fit(Train_X_Tfidf,Train_Y)
# predict the labels on validation dataset
predictions_NB = Naive.predict(Test_X_Tfidf)

print(Naive.score(Train_X_Tfidf, Train_Y))
# Use accuracy_score function to get the accuracy
print("Naive Bayes Accuracy Score: ",accuracy_score(predictions_NB, Test_Y)*100)
print("Naive Bayes Explained Variance Score: ",explained_variance_score(Test_Y, predictions_NB, multioutput='uniform_average'))

print ("")

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(Train_X_Tfidf, Train_Y)


print(clf.score(Train_X_Tfidf, Train_Y))
predictions_RF = clf.predict(Test_X_Tfidf)
print("Random Forest with 10 Estimators Accuracy Score: ",accuracy_score(predictions_RF, Test_Y)*100)
print("Random Forest Explained Variance Score: ",explained_variance_score(Test_Y, predictions_RF, multioutput='uniform_average'))

print ("")


from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

column_list = []

# Split the dataset into training and testing sets
Train_X, Test_X, Train_Y, Test_Y = train_test_split(merged['text_final'], merged['y'], test_size=0.3)

# Convert the text data into TF-IDF features
Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(merged['text_final'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

# Classifier - Algorithm - Neural Network (MLP)
nn = MLPClassifier(hidden_layer_sizes=(32, 128, 16), activation='relu', solver='adam', random_state=42)
nn.fit(Train_X_Tfidf, Train_Y)

# Predict the labels on the validation dataset
predictions_NN = nn.predict(Test_X_Tfidf)



print(nn.score(Train_X_Tfidf, Train_Y))
# Use accuracy_score function to get the accuracy
print("Neural Network Accuracy Score: ", accuracy_score(predictions_NN, Test_Y) * 100)
print("Neural Network Explained Variance Score: ", explained_variance_score(Test_Y, predictions_NN, multioutput='uniform_average'))


print ("")

from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

column_list = []

# Split the dataset into training and testing sets
Train_X, Test_X, Train_Y, Test_Y = train_test_split(merged['text_final'], merged['y'], test_size=0.3)

# Convert the text data into TF-IDF features
Tfidf_vect = TfidfVectorizer(max_features=5000)
Tfidf_vect.fit(merged['text_final'])
Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

# Classifier - Algorithm - Logistic Regression
lr = LogisticRegression(random_state=42)
lr.fit(Train_X_Tfidf, Train_Y)

# Predict the labels on the validation dataset
predictions_LR = lr.predict(Test_X_Tfidf)


print(lr.score(Train_X_Tfidf, Train_Y))

# Use accuracy_score function to get the accuracy
print("Logistic Regression Accuracy Score: ", accuracy_score(predictions_LR, Test_Y) * 100)
print("Logistic Regression Explained Variance Score: ", explained_variance_score(Test_Y, predictions_LR, multioutput='uniform_average'))

predictions_svm = SVM.predict(Test_X_Tfidf)
predictions_nb = Naive.predict(Test_X_Tfidf)
predictions_rf = clf.predict(Test_X_Tfidf)
predictions_nn = nn.predict(Test_X_Tfidf)
predictions_lr = lr.predict(Test_X_Tfidf)

from scipy.stats import mode

combined_predictions = []
for i in range(len(predictions_svm)):
    combined_prediction = mode([predictions_svm[i], predictions_nb[i], predictions_rf[i], predictions_nn[i], predictions_lr[i]])[0][0]
    combined_predictions.append(combined_prediction)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(Test_Y, combined_predictions)
print("Accuracy using Majority Vote:", accuracy)

weights = [explained_variance_score(Test_Y, predictions_svm, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_nb, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_rf, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_nn, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_lr, multioutput='uniform_average')]

weighted_predictions = []
for i in range(len(predictions_svm)):
    vote_count = {}
    for j, prediction in enumerate([predictions_svm[i], predictions_nb[i], predictions_rf[i], predictions_nn[i], predictions_lr[i]]):
        if prediction in vote_count:
            vote_count[prediction] += weights[j]
        else:
            vote_count[prediction] = weights[j]
    weighted_prediction = max(vote_count, key=vote_count.get)
    weighted_predictions.append(weighted_prediction)

accuracy = accuracy_score(Test_Y, weighted_predictions)
print("Accuracy using Weighted Majority Vote:", accuracy)

weights = [explained_variance_score(Test_Y, predictions_svm, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_nb, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_rf, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_nn, multioutput='uniform_average'),
           explained_variance_score(Test_Y, predictions_lr, multioutput='uniform_average')]

weighted_predictions = []
for i in range(len(predictions_svm)):
    vote_count = {}
    for j, prediction in enumerate([predictions_svm[i], predictions_nb[i], predictions_rf[i], predictions_nn[i], predictions_lr[i]]):
        if prediction in vote_count:
            vote_count[prediction] += weights[j]
        else:
            vote_count[prediction] = weights[j]
    weighted_prediction = max(vote_count, key=vote_count.get)
    weighted_predictions.append(weighted_prediction)

accuracy = accuracy_score(Test_Y, weighted_predictions)
print("Accuracy using Weighted Majority Vote:", accuracy)

# Print the weights assigned to each model
print("Weights assigned to each model:", weights)

# Determine the index of the model with the highest weight
most_important_model_index = weights.index(max(weights))

# Print the name of the most important model
model_names = ["SVM", "Naive Bayes", "Random Forest", "Neural Network", "Logistic Regression"]
print("The most important model is:", model_names[most_important_model_index])